# SLURM GRES configuration for softmig (Digital Research Alliance Canada)
# This file should be added to /etc/slurm/gres.conf

# Disable auto-detection (we define GPUs manually)
AutoDetect=off

# ===== PHYSICAL GPUs =====
# Keep your existing physical GPU definitions
# These are the actual hardware GPUs
NodeName=rack[01-14]-[01-16] Name=gpu Type=l40s File=/dev/nvidia[0-3]

# ===== LOGICAL GPU SLICES FOR softmig =====
# These are virtual GPU slices that softmig will enforce
# No File= parameter means SLURM doesn't bind to specific devices
# softmig handles the actual device assignment and limiting

# Full GPU slices (1x - no oversubscription, but still tracked)
NodeName=rack[01-14]-[01-16] Name=gpu Type=l40s.1 Count=4

# Half GPU slices (2x oversubscription - 2 jobs can share 1 GPU)
# Each job gets 24GB memory, 50% SM utilization
NodeName=rack[01-14]-[01-16] Name=gpu Type=l40s.2 Count=8

# Quarter GPU slices (4x oversubscription - 4 jobs can share 1 GPU)
# Each job gets 12GB memory, 25% SM utilization
NodeName=rack[01-14]-[01-16] Name=gpu Type=l40s.4 Count=16

# Eighth GPU slices (8x oversubscription - 8 jobs can share 1 GPU)
# Each job gets 6GB memory, 12.5% SM utilization
NodeName=rack[01-14]-[01-16] Name=gpu Type=l40s.8 Count=32

# Notes:
# - Count values should match oversubscription factor
# - l40s.2 Count=8 means 8 logical slices from 4 physical GPUs (2x oversubscription)
# - l40s.4 Count=16 means 16 logical slices from 4 physical GPUs (4x oversubscription)
# - l40s.8 Count=32 means 32 logical slices from 4 physical GPUs (8x oversubscription)
# - softmig enforces the actual memory and SM limits per job

